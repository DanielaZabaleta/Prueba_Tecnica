# -*- coding: utf-8 -*-
"""Prueba Técnica Empathy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13AqZNJYzgRol6Ce-jvZWO2lS4YMJkRw9

#Prueba Técnica: Ingeniero de Datos para Empathy Salud Contexto del Proyecto

##Objetivo del Ejercicio
Desarrollar un pipeline de datos completo que transforme datos crudos de evaluaciones médicas ocupacionales en insights accionables para identificar riesgos de salud laboral. Se puede incorporar técnicas de IA generativa para enriquecer el análisis.

##Descripción del Ejercicio
###Parte 1: Procesamiento y Preparación de Datos (40%) Tareas:
•	Realizar un análisis exploratorio completo del dataset proporcionado.


•	Implementar un pipeline de limpieza y transformación para manejar valores ausentes, normalizaciones y estandarizaciones necesarias.

•	Crear variables derivadas que puedan enriquecer el análisis (ej. categorización de riesgos basados en múltiples indicadores).

•	Documentar todos los pasos del procesamiento y decisiones tomadas.
###Parte 2: Realizar ejercicio de analítica de datos y modelación  (50%) Tareas:
•	Desarrollar visualizaciones efectivas que comuniquen patrones identificados entre factores de riesgo (estilo de vida, edad, área de trabajo) y problemas de salud.

•	Implementar validación cruzada y explicar la elección de métricas de evaluación.

•	Proponer potencial ejercicio de segmentación de datos que genere valor agregado al negocio.  

###Parte 3: Implementación de IA Generativa (10%)
En este ejercicio se permite hacer uso de IA para ayuda en la toma de decisiones en la actividad, podrá incorporar modelos de lenguaje (LLMs) para potenciar su solución. Esta es la parte más creativa del ejercicio.

##Parte 1: Procesamiento y Preparación de Datos
"""

#Librerias
#Importamos librerías básicas
from google.colab import files
import pandas as pd #manipulacion dataframes
import io
import numpy as np  #matrices y vectores
import matplotlib.pyplot as plt #gráfica
import seaborn as sns

#Buscamos el dataset dentro de nuestro explorador de archivos
uploaded = files.upload()

#Cargamos el dataset
df = pd.read_csv(io.BytesIO(uploaded['synthetic_health_data_new.csv']), sep=';')
df.head(10)

"""####Análisis exploratorio completo"""

#Información general del dataset
df.info()

#Cantidad de valores nulos por columna
df.isnull().sum()

"""####Pipeline de limpieza y transformación"""

#Transformaciónes del DataFrame
#Eliminamos columnas que no consideramos necesarias para el análisis
df.drop(['Fecha Nacimiento', 'Cedula', 'Fecha de Examen'], axis=1, inplace=True)

#Conversión de las columnas categóricas
categorical_columns = ['Genero', 'Estado civil', 'hemo', 'Escolaridad', 'Profesion', 'Estrato', 'Grupo Etareo', 'Sede', 'Area']
df[categorical_columns] = df[categorical_columns].astype('category')

#Reemplazo de los valores binarios (1/0) por valores booleanos (True/False)
binary_columns = ['Fuma', 'bebealcohol', 'Actifisica', 'siesta', 'SignosVitales_Dominancia']
df[binary_columns] = df[binary_columns].replace({1: True, 0: False})

#Correciónes logicas
df.loc[df['bebealcohol'] == False, 'Frec_alcohol'] = 'No bebe'

df.loc[df['Actifisica'] == False, 'Frec_actifisica'] = 'No realiza'
df.loc[df['Actifisica'] == False, 'Tipo_actifísica'] = 'No realiza'

df.loc[df['Fuma'] == False, 'TFumar'] = 'no fuma'

#Correcciones aplicadas
df[['bebealcohol', 'Frec_alcohol', 'Fuma', 'TFumar', 'Actifisica', 'Tipo_actifísica', 'Frec_actifisica']].head(10)

#Resumen estadístico de las columnas numéricas para determinar normalización y estandarización
df.describe()

#Estandarización de variables que tienen un rango amplio o varian siginificativamente
from sklearn.preprocessing import StandardScaler

columns_standardize = ['EDAD', 'SignosVitales_Peso', 'SignosVitales_IMC', 'Dias Perdidos']

scaler = StandardScaler()
df[columns_standardize] = scaler.fit_transform(df[columns_standardize])

df[columns_standardize].describe()

#Veamos nuestros datos transformados
df.head(10)

"""####Variables derivadas"""

#Variables derivadas para enriquecer el análisis
def Riesgo_General(row):
    if row['SignosVitales_IMCinterpretación'] == 'OBESIDAD TIPO I' or row['SignosVitales_Interpretaciónmedico'] == 'HTA Grado 1' or row['Fuma'] == True or row['bebealcohol'] == True:
        return 'Alto Riesgo'
    elif row['SignosVitales_IMCinterpretación'] == 'SOBREPESO' or row['SignosVitales_Interpretaciónmedico'] == 'PREHIPERTENSION':
        return 'Riesgo Moderado'
    else:
        return 'Bajo Riesgo'

#Creación de la variable de Riesgo General
df['Riesgo_General'] = df.apply(Riesgo_General, axis=1)

#Mostramos los resultados
df[['SignosVitales_IMCinterpretación', 'SignosVitales_Interpretaciónmedico', 'Fuma', 'bebealcohol', 'Riesgo_General']].head(10)

#Variables derivadas para enriquecer el análisis
umbral_dias_perdidos = 10

#Nueva variable que combina el riesgo general con los días perdidos
umbral_bajo_dias = 0  #Media (0)
umbral_alto_dias = 1  #1 desviación estándar por encima de la media

def Categoria_Ausentismo(row):
    #Muchos días perdidos (>= 1 desviación estándar)
    if row['Dias Perdidos'] >= umbral_alto_dias:
        if row['Riesgo_General'] == 'Alto Riesgo':
            return 'Alto riesgo de ausentismo por salud'
        elif row['Riesgo_General'] == 'Riesgo Moderado':
            return 'Riesgo moderado de ausentismo por salud'
        elif row['Riesgo_General'] == 'Bajo Riesgo':
            return 'Inconsistente muchos días con bajo riesgo'

    #Pocos días perdidos (entre la media y 1 desviación estándar)
    elif umbral_bajo_dias <= row['Dias Perdidos'] < umbral_alto_dias:
        if row['Riesgo_General'] == 'Alto Riesgo':
            return 'Alto riesgo con pocos días de ausentismo'
        elif row['Riesgo_General'] == 'Riesgo Moderado':
            return 'Riesgo moderado con pocos días de ausentismo'
        else:
            return 'Bajo riesgo con pocos días de ausentismo'

    #Muy pocos días perdidos (por debajo de la media)
    else:
        if row['Riesgo_General'] == 'Alto Riesgo':
            return 'Alto riesgo sin ausentismo'
        elif row['Riesgo_General'] == 'Riesgo Moderado':
            return 'Riesgo moderado sin ausentismo'
        else:
            return 'Bajo riesgo sin ausentismo'

df['Categoria_Ausentismo'] = df.apply(Categoria_Ausentismo, axis=1)

#Verificamos los resultados
df[['Dias Perdidos', 'Riesgo_General', 'Categoria_Ausentismo']].head(20)

#Variables derivadas para enriquecer el análisis
def Carga_Familiar(dependientes):
    if dependientes == 0:
        return 'Sin carga familiar'
    elif 1 <= dependientes <= 3:
        return 'Carga familiar moderada'
    else:
        return 'Alta carga familiar'

df['Carga_Familiar'] = df['Ndependientes'].apply(Carga_Familiar)

#Mostramos los resultados
df[['Ndependientes', 'Carga_Familiar']].head(10)

#Conversión de las columnas categóricas
categorical_columns_new = ['Carga_Familiar', 'Categoria_Ausentismo', 'Riesgo_General']
df[categorical_columns_new] = df[categorical_columns_new].astype('category')

"""##Parte 2: Realizar ejercicio de analítica de datos y modelación

####Visualizaciones efectivas que comuniquen patrones identificados entre factores de riesgo y problemas de salud.
"""

#Matriz de correlacion para variables numericas

df_numeric = df.select_dtypes(include=['float64', 'int64'])

plt.figure(figsize=(25, 15))
sns.heatmap(df_numeric.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

# prompt: Matriz de correlacion para variables numericas

import matplotlib.pyplot as plt
import numpy as np
# Selecciona solo las variables numéricas para la matriz de correlación
numeric_cols = df.select_dtypes(include=np.number).columns

# Calcula la matriz de correlación
correlation_matrix = df[numeric_cols].corr()

# Muestra la matriz de correlación
print(correlation_matrix)

# Visualiza la matriz de correlación con un mapa de calor
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

#Correlaciónes lineales
#Convertimos a número todas las variables

#Dummies para variable con más de 2 categorías y con 2 categorías
df_corr = pd.get_dummies(df, columns=['Carga_Familiar', 'Categoria_Ausentismo', 'Riesgo_General', 'Estado civil', 'hemo',
                                        'Escolaridad', 'Grupo Etareo', 'Sede', 'Area','Genero', 'Fuma', 'bebealcohol', 'Actifisica',
                                      'TFumar', 'Frec_actifisica', 'Frec_alcohol'], drop_first=True, dtype=float)

df_corr = df_corr.select_dtypes(include=['number'])

df_corr.head(5)

# Correlaciones
corr_lin = df_corr.corr()
corr_lin

#Matriz de correlación
plt.figure(figsize=(40, 25))
sns.heatmap(corr_lin.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.title('Matriz de Correlación de Variables')
plt.tight_layout()
plt.show()

#Gráficos de dispersión para comparar factores con los días perdidos
variables_comparar = ['SignosVitales_IMC', 'EDAD']

fig, axes = plt.subplots(1, 2, figsize=(18, 5))
for i, var in enumerate(variables_comparar):
    sns.scatterplot(x=df[var], y=df['Dias Perdidos'], ax=axes[i])
    axes[i].set_title(f'Dispersión de {var} vs. Días Perdidos')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Días Perdidos')

#Distribución de días perdidos por nivel de riesgo de salud
plt.figure(figsize=(10, 6))
sns.boxplot(x='Riesgo_General', y='Dias Perdidos', data=df)
plt.title('Distribución de Días Perdidos por Nivel de Riesgo General')
plt.xlabel('Nivel de Riesgo General')
plt.ylabel('Días Perdidos')
plt.show()

#Relación entre el nivel de actividad física y el área de trabajo
plt.figure(figsize=(10, 6))
sns.countplot(x='Area', hue='Frec_actifisica', data=df)
plt.title('Distribución del Nivel de Actividad Física por Área de Trabajo')
plt.xticks(rotation=45)
plt.xlabel('Área de Trabajo')
plt.ylabel('Número de Trabajadores')
plt.show()

#Análisis de áreas de trabajo y variación de riesgos
plt.figure(figsize=(12, 6))
sns.countplot(x='Area', hue='Riesgo_General', data=df)
plt.title('Variación de Riesgos según el Área de Trabajo')
plt.xlabel('Área de Trabajo')
plt.ylabel('Cantidad')
plt.xticks(rotation=90)
plt.show()

"""####Validación Cruzada"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error, r2_score

#Modelo para predecir la cantidad de días perdidos
#Variables explicativas (factores de riesgo)
Variables_modelo = [
    'EDAD', 'SignosVitales_IMC', 'Frec_actifisica', 'Frec_alcohol', 'Tantefumar',
    'Enfermedades del aparato circulatorio', 'Enfermedades endocrinas, nutricionales y metabólicas']
#Variable objetivo
target = 'Dias Perdidos'


df_model = df[Variables_modelo + [target]].dropna()
df_model.head(5)

#Convertimos las variables categóricas usando One-Hot Encoding
df_model_encoded = pd.get_dummies(df_model, columns=['Frec_actifisica', 'Frec_alcohol'], drop_first=True)

#Dividimos el df en conjunto de entrenamiento y prueba
X = df_model_encoded.drop(columns=[target])
y = df_model_encoded[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)

#Modelo
from xgboost import XGBRegressor

#Entrenamiento
xgb_model = XGBRegressor(n_estimators=30, max_depth=3, random_state=42)
xgb_model.fit(X_train, y_train)

#Predecicción
y_pred_xgb = xgb_model.predict(X_test)

#Evaluamos el modelo
cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
cv_mse_scores = -cv_scores
cv_mse_scores.mean(), cv_mse_scores.std()

"""####Segmentación de datos"""

from sklearn.cluster import KMeans

variables_clustering = ['SignosVitales_IMC', 'EDAD', 'Dias Perdidos']
df_clustering = df[variables_clustering].dropna()

kmeans = KMeans(n_clusters=3, random_state=42)
df_clustering['Cluster'] = kmeans.fit_predict(df_clustering)
centroides = kmeans.cluster_centers_

scaler_clustering = StandardScaler()

# Fit the new scaler only with the columns used for clustering
scaler_clustering.fit(df[['SignosVitales_IMC', 'EDAD', 'Dias Perdidos']])

# Apply inverse_transform using the new scaler
centroides_original = scaler_clustering.inverse_transform(centroides)

#Visualizar los grupos mediante un gráfico de dispersión
plt.figure(figsize=(10, 6))
sns.scatterplot(x=df_clustering['EDAD'], y=df_clustering['SignosVitales_IMC'], hue=df_clustering['Cluster'], palette='viridis', marker='o')
plt.scatter(centroides_original[:, 1], centroides_original[:, 0], c='red', s=200, marker='X')  # Centroides en la gráfica
plt.title('Segmentación de Empleados según IMC, Edad y Días Perdidos')
plt.xlabel('Edad')
plt.ylabel('IMC')
plt.show()

print("Centroides de los Clusters en la escala original:")
print(pd.DataFrame(centroides_original, columns=['IMC', 'Edad', 'Días Perdidos']))